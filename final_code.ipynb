{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAZK6zKHiTl6"
      },
      "source": [
        "# Normal Code Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHTleuF0iKuv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "from datetime import datetime\n",
        "import os\n",
        "import psutil\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoded vectors\n",
        "num_classes = 10\n",
        "train_labels = to_categorical(train_labels, num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=train_images.shape[1:]))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Get the RAM usage of the current process\n",
        "process = psutil.Process(os.getpid())\n",
        "memory_usage = process.memory_info().rss / 1024 / 1024  # in MB\n",
        "\n",
        "print(\"RAM usage: \", memory_usage, \" MB\")\n",
        "\n",
        "# Calculate the total training time in seconds\n",
        "total_training_time = end_time - start_time\n",
        "\n",
        "print(\"Total training time: \", total_training_time, \" seconds.\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Test accuracy:', test_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmCBkT3LiW-z"
      },
      "source": [
        "# layer-to-layer training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2BfE5RkiPtI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import time\n",
        "import os\n",
        "import psutil\n",
        "\n",
        "num_classes = 10\n",
        "# Load CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize the input data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# One-hot encode the output labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "# model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "\n",
        "models = []\n",
        "# Train the model for each set of layers\n",
        "num_layers = len(model.layers)\n",
        "for i in range(num_layers // 2):\n",
        "    # Choose the layers to train\n",
        "    first_layer = i\n",
        "    last_layer = num_layers - i - 1\n",
        "    \n",
        "    # Freeze all layers except the chosen ones\n",
        "    for j, layer in enumerate(model.layers):\n",
        "        if j < first_layer or j > last_layer:\n",
        "            layer.trainable = False\n",
        "        else:\n",
        "            layer.trainable = True\n",
        "    # Define the model checkpoint\n",
        "    # checkpoint_path = f\"model_checkpoint_{i}.h5\"\n",
        "    # model_checkpoint = ModelCheckpoint(\n",
        "    #     checkpoint_path,\n",
        "    #     save_best_only=True,\n",
        "    #     save_weights_only=True,\n",
        "    #     monitor='val_accuracy',\n",
        "    #     mode='max',\n",
        "    #     verbose=1\n",
        "    # )\n",
        "    # Compile the model with frozen and trainable layers\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(lr=0.001),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # Train the model for 10 epochs with the chosen layers\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=128,\n",
        "              epochs=10,\n",
        "              validation_data=(x_test, y_test))\n",
        "              # callbacks=[model_checkpoint])\n",
        "    models.append(model)\n",
        "    # Print the accuracies for each epoch\n",
        "    history = model.history.history\n",
        "    print(f\"Training accuracy after {i+1} set of layers: {history['accuracy']}\")\n",
        "    print(f\"Validation accuracy after {i+1} set of layers: {history['val_accuracy']}\")\n",
        "\n",
        "    # # Evaluate the model\n",
        "    # loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    # print(\"Accuracy for model with layers %d to %d: %.2f%%\" % (first_layer, last_layer, acc * 100))\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "# Get the RAM usage of the current process\n",
        "process = psutil.Process(os.getpid())\n",
        "memory_usage = process.memory_info().rss / 1024 / 1024  # in MB\n",
        "\n",
        "print(\"RAM usage: \", memory_usage, \" MB\")\n",
        "# Calculate the total training time in seconds\n",
        "total_training_time = end_time - start_time\n",
        "\n",
        "print(\"Total training time: \", total_training_time, \" seconds.\")\n",
        "\n",
        "# Ensemble all models\n",
        "num_models = len(models)\n",
        "predictions = []\n",
        "for i in range(num_models):\n",
        "    model = models[i]\n",
        "    y_pred = model.predict(x_test)\n",
        "    predictions.append(y_pred)\n",
        "\n",
        "# Take average of all predictions\n",
        "y_pred_ensemble = sum(predictions) / num_models\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "accuracy.update_state(y_test, y_pred_ensemble)\n",
        "print(\"Ensemble accuracy:\", accuracy.result().numpy())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
